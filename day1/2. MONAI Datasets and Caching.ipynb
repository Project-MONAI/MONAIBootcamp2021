{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"monai.png\" style=\"width: 700px;\"/>\n",
    "\n",
    "Welcome to the MONAI bootcamp! This notebook will introduce you to the MONAI Caching, Datasets and Network options, and then hands-on  and architecture, and then hands-on with implmenting SmartCache on a test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Google Colab\n",
    "\n",
    "This notebook has the pip command for installing MONAI and will be added to any subsequent notebook.\n",
    "\n",
    "**Required Packages for Colab Execution**\n",
    "\n",
    "Execute the following cell to install MONAI the first time a colab notebook is run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \"monai[ignite, nibabel, torchvision, tqdm]==0.6.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enabling GPU Support**\n",
    "\n",
    "To use GPU resources through Colab, change the runtime to GPU:\n",
    "\n",
    "1. From the **\"Runtime\"** menu select **\"Change Runtime Type\"**\n",
    "2. Choose **\"GPU\"** from the drop-down menu\n",
    "3. Click **\"SAVE\"**\n",
    "\n",
    "This will reset the notebook and probably ask you if you are a robot (these instructions assume you are not). Running\n",
    "\n",
    "**!nvidia-smi**\n",
    "\n",
    "in a cell will verify this has worked and show you what kind of hardware you have access to.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding MONAI Datasets, Caching, and Networks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users often need to train the model with many (potentially thousands of) epochs over the data to achieve the desired model quality. A native PyTorch implementation may repeatedly load data and run the same preprocessing steps for every epoch during training, which can be time-consuming and unnecessary, especially when the medical image volumes are large.  By utilizing Dataset Caching, you can reduce the amount of time your system takes to load this data and preprocess it, reducing your overall training time.\n",
    "\n",
    "Network functionality represents a significant design opportunity for MONAI. Pytorch is very much unopinionated in how networks are defined. It provides Module as a base class to create a network and a few methods that must be implemented. Still, there is no prescribed pattern nor much helper functionality for initializing networks.\n",
    "\n",
    "The lack of helper functionality leaves a lot of room for defining some beneficial 'best practice' patterns for constructing new networks in MONAI. Although trivial, inflexible network implementations are easy enough, but we can give users a toolset that makes it much easier to build well-engineered, flexible networks and demonstrate their value by committing to use them in the networks that we build.\n",
    "\n",
    "## MONAI Datasets, Caching, and Networks\n",
    "\n",
    "To help you understand more about MONAI Datasets and Caching options, this guide will help you answer five key questions:\n",
    "\n",
    "1. **What is a MONAI Dataset?**\n",
    "2. **What is Dataset Caching and how do I use it?**\n",
    "3. **What common datasets are provided by MONAI?**\n",
    "4. **What Network and Network components does MONAI provide?**\n",
    "5. **How do you use MONAI Layers?**\n",
    "6. **How do you use these flexible layers to create a network?**\n",
    "7. **Which Networks are included with MONAI?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's get started by importing our dependencies.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.6.0\n",
      "Numpy version: 1.20.3\n",
      "Pytorch version: 1.8.1+cu102\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 0ad9e73639e30f4f1af5a1f4a45da9cb09930179\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.4\n",
      "Nibabel version: 3.2.1\n",
      "scikit-image version: 0.18.1\n",
      "Pillow version: 8.2.0\n",
      "Tensorboard version: 2.5.0\n",
      "gdown version: 3.13.0\n",
      "TorchVision version: 0.9.1+cu102\n",
      "ITK version: 5.1.2\n",
      "tqdm version: 4.61.0\n",
      "lmdb version: 1.2.1\n",
      "psutil version: 5.8.0\n",
      "pandas version: 1.2.4\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.data import Dataset, DataLoader, CacheDataset, PersistentDataset, SmartCacheDataset\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.transforms import (\n",
    "    MapTransform,\n",
    ")\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. What is a MONAI Dataset?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MONAI Dataset is a generic dataset with a __len__ property, __getitem__ property, and an optional callable data transform when fetching a data sample.\n",
    "\n",
    "We'll start by initializing some generic data, calling the Dataset class with the generic data, and specifying None for our transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [{\"data\": 4}, \n",
    "         {\"data\": 9}, \n",
    "         {\"data\": 3}, \n",
    "         {\"data\": 7}, \n",
    "         {\"data\": 1},\n",
    "         {\"data\": 2},\n",
    "         {\"data\": 5}]\n",
    "dataset = monai.data.Dataset(items, transform=None)\n",
    "\n",
    "print(f\"Length of dataset is {len(dataset)}\")\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compatible with the PyTorch DataLoader\n",
    "\n",
    "MONAI functionality should be compatible with the PyTorch DataLoader, although free to subclass from it if there is additional functionality that we consider key, which cannot be realized with the standard DataLoader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in torch.utils.data.DataLoader(dataset, batch_size=2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load items with a customized transform\n",
    "\n",
    "We'll create a custom transform called `SquareIt`, which will replace the corresponding value of the input's `keys` with a squared value. In our case, `SquareIt(keys='data')` will apply the square transform to the value of `x['data']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareIt(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        MapTransform.__init__(self, keys)\n",
    "        print(f\"keys to square it: {self.keys}\")\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        key = self.keys[0]\n",
    "        data = x[key]\n",
    "        output = {key: data ** 2}\n",
    "        return output\n",
    "\n",
    "square_dataset = Dataset(items, transform=SquareIt(keys='data'))\n",
    "for item in square_dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. What is Dataset Caching and how do I use it?**\n",
    "\n",
    " MONAI provides multi-thread versions of `CacheDataset` and `LMDBDataset` to accelerate these transformation steps during training by storing the intermediate outcomes before the first randomized transform in the transform chain. Enabling this feature could potentially give 10x training speedups in the Datasets experiment.\n",
    " \n",
    "<img src=\"cache_dataset.png\" style=\"width: 700px;\"/>\n",
    " \n",
    "To demonstrate the benefit dataset caching, we're going to construct a dataset with a slow transform.  To do that, we're going to call the sleep function during each of the `__call__` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlowSquare(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        MapTransform.__init__(self, keys)\n",
    "        print(f\"keys to square it: {self.keys}\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        time.sleep(1.0)\n",
    "        output = {key: x[key] ** 2 for key in self.keys}\n",
    "        return output\n",
    "\n",
    "square_dataset = Dataset(items, transform=SlowSquare(keys='data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it's going to take about 7 seconds to go through all the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time for item in square_dataset: print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time we run this loop we're going to get roughly 7 seconds to go through all of the items.  If you were do this for 100 epochs, you're adding almost 12 extra minutes of load time to your total training loop.  Let's look at ways that we can improve this time by utilizing caching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Dataset\n",
    "\n",
    "When using [CacheDataset](https://docs.monai.io/en/latest/data.html?highlight=dataset#cachedataset) the caching is done when the object is initialized for the first time, so the initialization is slower than a regular dataset.\n",
    "\n",
    "By caching the results of non-random preprocessing transforms, it accelerates the training data pipeline. If the requested data is not in the cache, all transforms will run normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_cached = CacheDataset(items, transform=SlowSquare(keys='data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, repeatedly fetching the items from an initialized CacheDataset is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit list(item for item in square_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistent Caching\n",
    "\n",
    "[PersistentDataset](https://docs.monai.io/en/latest/data.html?highlight=dataset#persistentdataset) allows for persistent storage of pre-computed values to efficiently manage larger than memory dictionary format data.\n",
    "\n",
    "The non-random transform components are computed when first used and stored in the cache_dir for rapid retrieval on subsequent uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_persist = monai.data.PersistentDataset(items, transform=SlowSquare(keys='data'), cache_dir=\"my_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time for item in square_persist: print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the initialization of the PersistentDataset we passed in the parameter \"my_cache\" for the location to store the intermediate data. We'll look at that directory below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls my_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling out to the dataset on the following epochs, it will not call the slow transform but used the cached data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit [item for item in square_persist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fresh dataset instances can make use of the caching data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_persist_1 = monai.data.PersistentDataset(items, transform=SlowSquare(keys='data'), cache_dir=\"my_cache\")\n",
    "%timeit [item for item in square_persist_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caching in action\n",
    "- There's also a [SmartCacheDataset](https://docs.monai.io/en/latest/data.html#monai.data.SmartCacheDataset) to hide the transforms latency with less memory consumption.\n",
    "- The dataset tutorial notebook has a working example and a comparison of different caching mechanism in MONAI: https://github.com/Project-MONAI/tutorials/blob/master/acceleration/dataset_type_performance.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"datasets_speed.png\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. What common datasets are provided by MONAI?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly get started with popular training data in the medical domain, MONAI provides several data-specific Datasets(like: MedNISTDataset, DecathlonDataset, etc.), which include downloading from our AWS storage, extracting data files and support generation of training/evaluation items with transforms.\n",
    "\n",
    "The [DecathlonDataset](https://docs.monai.io/en/latest/data.html?highlight=dataset#decathlon-datalist) function leverages the features described throughout this notebook.  These datasets are an extension of CacheDataset covered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = monai.apps.DecathlonDataset(root_dir=\"./\", task=\"Task09_Spleen\", section=\"training\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.get_properties(\"numTraining\"))\n",
    "print(dataset.get_properties(\"description\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0]['image'].shape)\n",
    "print(dataset[0]['label'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **What Network and Network components does MONAI provide?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MONAI provides definitions for networks and their components that inherit directly from Pytorch Module, Sequential, etc. These general purpose networks include parameterized topologies that can easily be expanded are independent from rest of MONAI so networks can be used with existing training code.\n",
    "\n",
    "MONAI includes the following submodules:\n",
    "- layers: defines low level layers, factories for selecting Pytorch and custom layers based on dimension and other arguments\n",
    "- blocks: mid-level building blocks defining specific reusable concepts networks are constructed from\n",
    "- nets: full network definitions for common architectures, eg. UNet, VNet, Densenet,\n",
    "\n",
    "Blocks and networks use LayerFactory objects as generic factory for custom and PyTorch layers.\n",
    "\n",
    "MONAI provides blocks for defining:\n",
    "- Convolution with activation and regularization\n",
    "- Residual units\n",
    "- Squeeze/excitation\n",
    "- Downsampling/upsampling\n",
    "- Subpixel convolutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you use MONAI Layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.layers import Conv, Act, split_args, Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution as an example\n",
    "\n",
    "The [Conv](https://docs.monai.io/en/latest/networks.html#convolution) class has two options for the first argument. The second argument must be the number of spatial dimensions, `Conv[name, dimension]`, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Conv[Conv.CONV, 1])\n",
    "print(Conv[Conv.CONV, 2])\n",
    "print(Conv[Conv.CONV, 3])\n",
    "print(Conv[Conv.CONVTRANS, 1])\n",
    "print(Conv[Conv.CONVTRANS, 2])\n",
    "print(Conv[Conv.CONVTRANS, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configured classes are the \"vanilla\" PyTorch layers. We could create instances of them by specifying the layer arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Conv[Conv.CONV, 2](in_channels=1, out_channels=4, kernel_size=3))\n",
    "print(Conv[Conv.CONV, 3](in_channels=1, out_channels=4, kernel_size=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Act](https://docs.monai.io/en/latest/networks.html#module-monai.networks.layers.Act) classes don't require the spatial dimension information, but supports additional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Act[Act.PRELU])\n",
    "Act[Act.PRELU](num_parameters=1, init=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These could be fully specified with a tuple of `(type_name, arg_dict)`, such as `(\"prelu\", {\"num_parameters\": 1, \"init\": 0.1})`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_name, act_args = split_args((\"prelu\", {\"num_parameters\": 1, \"init\": 0.1}))\n",
    "Act[act_name](**act_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. How do you use these components to create a network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flexible Definition Networks\n",
    "\n",
    "These APIs allow for flexible definitions of networks.  Below we'll create a class called `MyNetwork` that utilizes `Conv`, `Act`, and `Pool`.  Each Network requires an `__init__` and a `forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(torch.nn.Module):\n",
    "    \n",
    "  def __init__(self, dims=3, in_channels=1, out_channels=8, kernel_size=3, pool_kernel=2, act=\"relu\"):\n",
    "    super(MyNetwork, self).__init__()\n",
    "    # convolution\n",
    "    self.conv = Conv[Conv.CONV, dims](in_channels, out_channels, kernel_size=kernel_size)\n",
    "    # activation\n",
    "    act_type, act_args = split_args(act)\n",
    "    self.act = Act[act_type](**act_args)\n",
    "    # pooling\n",
    "    self.pool = Pool[Pool.MAX, dims](pool_kernel)\n",
    "  \n",
    "  def forward(self, x: torch.Tensor):\n",
    "    x = self.conv(x)\n",
    "    x = self.act(x)\n",
    "    x = self.pool(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network definition can be instantiated to support either 2D or 3D inputs, with flexible kernel sizes.  It becomes handy when adapting the same architecture design for different tasks, switching among 2D, 2.5D, 3D easily.\n",
    "\n",
    "Almost all the MONAI layers, blocks and networks are extensions of `torch.nn.modules` and follow this pattern. This makes the implementations compatible with any PyTorch pipelines and flexible with the network design. The current collections of those differentiable modules are listed in https://docs.monai.io/en/latest/networks.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default network instance\n",
    "default_net = MyNetwork()\n",
    "print(default_net)\n",
    "print(default_net(torch.ones(3, 1, 20, 20, 30)).shape)\n",
    "\n",
    "# 2D network instance\n",
    "elu_net = MyNetwork(dims=2, in_channels=3, act=(\"elu\", {\"inplace\": True}))\n",
    "print(elu_net)\n",
    "print(elu_net(torch.ones(3, 3, 24, 24)).shape)\n",
    "\n",
    "# 3D network instance with anisotropic kernels\n",
    "sigmoid_net = MyNetwork(3, in_channels=4, kernel_size=(3, 3, 1), act=\"sigmoid\")\n",
    "print(sigmoid_net)\n",
    "print(sigmoid_net(torch.ones(3, 4, 30, 30, 5)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MONAI provides over 20 Networks including:\n",
    "- UNet\n",
    "- VNet\n",
    "- AHNet\n",
    "- VGG-like regressor, classifier, discriminator, critic\n",
    "- HighResNet\n",
    "- SENet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example UNets\n",
    "\n",
    "We'll define a 2D UNet network with 2 hidden layers having outputs with 8 channels, and a bottom (bottleneck) layer producing outputs with 32 channels.  The stride values state the stride for the initial convolution, ie. downsampling in down path and upsampling in up path and it'll transpose the convolutions used to implement upsampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = monai.networks.nets.UNet(\n",
    "    dimensions=2,  # 2 or 3 for a 2D or 3D network\n",
    "    in_channels=1,  # number of input channels\n",
    "    out_channels=1,  # number of output channels\n",
    "    channels=[8, 16, 32],  # channel counts for layers\n",
    "    strides=[2, 2]  # strides for mid layers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll define a 4–layer 3D UNet with leaky ReLU activation in place of default PReLU.  THis time we'll instantiate the Act parameter which is the activation layer factory, and recalls names of known activation layers, eg. LEAKYRELU.  You can add custom layers to all factories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = monai.networks.nets.UNet(\n",
    "    dimensions=3,  \n",
    "    in_channels=1,      \n",
    "    out_channels=1,  \n",
    "    channels=[8, 16, 32, 64],\n",
    "    strides=[2, 2, 2],\n",
    "    act=monai.networks.layers.Act.LEAKYRELU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflows\n",
    "\n",
    "MONAI includes extensions to Ignite Engine classes.  These workflow objects encompass the majority of the training process and provide default training loops and mechanism for responding to events.  This helps reduce code complexity and amount to be written for each experiment and are totally optional modules, other frameworks such as Lightning or Catalyst can be used instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary**\n",
    "\n",
    "We've covered MONAI Datasets, Caching and Networks.  Here are some key highlights:\n",
    "\n",
    "- A MONAI Dataset is a generic dataset with a len property, getitem property, and an optional callable data transform when fetching a data sample.\n",
    "- You can use dataset caching to store dataset transforms to speed up training.  Some included Caching options are CachingDataset, PersistentCaching, and SmartCaching\n",
    "- MONAI provides access to some commonly used medical imaging datasets including the DecathlonDataset\n",
    "- Understanding the basic MONAI Layers, Blocks, and Networks\n",
    "- Use MONAI layers to implement a flexible network and instnaite a two UNet examples with different parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Assignment 1**\n",
    "\n",
    "### Instantiate a Dataset with SmartCache\n",
    "\n",
    "Requirements: \n",
    "- Write a transform that subtracts 1 from the input value\n",
    "- Combine the new Transform with the existing SlowSquare from above\n",
    "- Instantiate SmartCacheDataset with replacement rate of .2 and cache number of 5\n",
    "- Run SmartCache 5-times so that you can see the replacement values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import SmartCacheDataset\n",
    "from monai.transforms import Compose, MapTransform\n",
    "\n",
    "class MinusOne(MapTransform):\n",
    "    # constructor needed here?\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        pass # write code here\n",
    "\n",
    "smart_transform = Compose([\n",
    "    # ???\n",
    "])    \n",
    "\n",
    "smart_square = # ???\n",
    "\n",
    "%time for item in smart_square: print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will utilize SmartCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run through SmartCache replacement N-times\n",
    "smart_square.start()\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"\\nCache: \", i)\n",
    "    for item in smart_square: \n",
    "        print(item)\n",
    "    smart_square.update_cache()\n",
    "    \n",
    "smart_square.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Assignment 2**\n",
    "\n",
    "Instantitate a network from the MONAI library. Inspect the output to see which composable layers were used in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some networks, such as UNet, are defined using template methods to create their layers. Experiment with extending one of these classes to change the composition of the created network by overriding these methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next notebook, we cover a MONAI end-to-end workflow.\n",
    "\n",
    "You can find more information about everything covered here on our [MONAI Documentation Page](https://docs.monai.io/).  \n",
    "\n",
    "If you're looking for more examples and tutorials, we have a repo dedicated just to that!  You can find it on our [GitHub Organization Page](https://github.com/Project-MONAI/tutorials)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:monai]",
   "language": "python",
   "name": "conda-env-monai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
